{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from wordcloud import WordCloud, get_single_color_func\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "# pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 new posts of the since 2/14/24\n",
    "df_new = pd.read_pickle('data/askreddit_new_reddit_data_02_14_2024.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['title'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['selftext'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp = df_new[['id', 'title', 'selftext', 'datetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the title and body of the post\n",
    "\n",
    "df_new_nlp['text_comb'] = df_new_nlp['title'] + '. ' + df_new_nlp['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    \n",
    "    # Make everything lowercase\n",
    "    cleaned = text.lower()\n",
    "\n",
    "    # Remove excessive white space and newlines\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \")\n",
    "    cleaned = re.sub(pattern=r' {2,}', repl=' ', string=cleaned)\n",
    "\n",
    "    # Ensure apostrophes and quotation marks are consistent\n",
    "    cleaned = re.sub(r\"’|‘\", repl=\"'\", string=cleaned)\n",
    "    cleaned = re.sub(r\"“|”\", repl='\"', string=cleaned)\n",
    "\n",
    "    # Remove any difficult characters like emojis, elipses, etc. \n",
    "    cleaned = ''.join(filter(lambda x: x in string.printable, cleaned))\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb'] = df_new_nlp['text_comb'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not remove punctuation in the cleaning step?\n",
    "\n",
    "Removing punctuation is a common step in cleaning text for NLP. I don't do it here since I plan to lemmatize words which requires keeping grammatical context in the text. Punctuation will be removed in a future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Spacy and NLTK stop words to make them more comperehensive\n",
    "\n",
    "sw_spacy = list(nlp.Defaults.stop_words)\n",
    "sw_nltk = stopwords.words('english')\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = list(set(sw_spacy + sw_nltk + punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text_comb column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a Spacy object for each title\n",
    "\n",
    "df_new_nlp['spacy_text_comb'] = df_new_nlp['text_comb'].apply(lambda x: nlp(clean_string(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_nlp.sample()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb'].loc[770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Spacy's nlp object works:\n",
    "\n",
    "sample_title = df_new_nlp['title'].loc[770]\n",
    "\n",
    "display('Spacy Object:')\n",
    "display(nlp(sample_title))\n",
    "print('Type:')\n",
    "print(type(nlp(sample_title)))\n",
    "print()\n",
    "print('Tokens:')\n",
    "print([t.text for t in list(nlp(sample_title))])\n",
    "print()\n",
    "print('Lemmas:')\n",
    "print([t.lemma_ for t in list(nlp(sample_title))])\n",
    "print()\n",
    "print('Detect punctuation, numbers, etc.:')\n",
    "print([t.is_punct for t in list(nlp(sample_title))])\n",
    "print([t.is_digit for t in list(nlp(sample_title))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(spacy_doc):\n",
    "    \"\"\"\n",
    "    Takes in a Spacy doc and converts it to a list of tokens.\n",
    "    Omits punctuation, non-ASCII characters, digits, URLs.\n",
    "    \"\"\"\n",
    "    token_mask = \\\n",
    "        lambda x: all(\n",
    "            [x.is_ascii, not x.like_url, not x.is_digit, not x.is_punct]\n",
    "            )\n",
    "    tokens = [t.text for t in spacy_doc if token_mask(t)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb_tokens'] = df_new_nlp['spacy_text_comb'].apply(spacy_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens. Stop words color-coded.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "top_50_tok = OrderedDict(\n",
    "    FreqDist(df_new_nlp['text_comb_tokens'].explode()).most_common(50)\n",
    "    )\n",
    "\n",
    "tokens = list(top_50_tok.keys())\n",
    "freq = list(top_50_tok.values())\n",
    "not_sw = [t for t in tokens if t not in stopword_list]\n",
    "ax.bar(x=tokens, height=freq, color=['#f14848' if t in not_sw else '#2c2fbf' for t in tokens])\n",
    "ax.set_ylabel('Frequency', size=10)\n",
    "ax.set_xlabel('Tokens', size=10)\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_title('Top 50 tokens in r/askreddit\\n2/9 to 2/14 2024')\n",
    "custom_bars = [Rectangle((0,0),1,1,color=c, alpha=1) for c in ['#2c2fbf', '#f14848']]\n",
    "ax.legend(custom_bars, ['In stop words','Not in stop words'], fontsize=10)\n",
    "fig.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/top_50_tokens2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb_tokens_no_sw'] = df_new_nlp['text_comb_tokens'].apply(lambda x: [t for t in x if t not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing most frequent tokens without stopwords:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "top_50_tok = OrderedDict(\n",
    "    FreqDist(df_new_nlp[df_new_nlp['text_comb_tokens_no_sw'].apply(lambda x: len(x) > 0)]['text_comb_tokens_no_sw'].explode()).most_common(50)\n",
    "    )\n",
    "\n",
    "tokens = list(top_50_tok.keys())\n",
    "freq = list(top_50_tok.values())\n",
    "ax.bar(x=tokens, height=freq, color='#f14848')\n",
    "ax.set_xlabel('Tokens', size=10)\n",
    "ax.set_ylabel('Frequency', size=10)\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "ax.set_title('Top 50 tokens in r/askreddit (no stop words)\\n2/9 to 2/14 2024')\n",
    "fig.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/top_50_tokens_no_sw2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(spacy_doc):\n",
    "    \"\"\"\n",
    "    Takes in a Spacy doc and converts it to a list of lemmas.\n",
    "    Omits punctuation, non-ASCII characters, digits, URLs.\n",
    "    \"\"\"\n",
    "    token_mask = \\\n",
    "        lambda x: all(\n",
    "            [x.is_ascii, not x.like_url, not x.is_digit, not x.is_punct]\n",
    "            )\n",
    "    lemmas = [t.lemma_.lower() for t in spacy_doc if token_mask(t)]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp['text_comb_lemmas'] = df_new_nlp['spacy_text_comb'].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check most common lemmas\n",
    "\n",
    "FreqDist(df_new_nlp['text_comb_lemmas'].explode()).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas without stop words\n",
    "\n",
    "In order to make the lemmatized tokens without stop words, I will also need to lemmatize the stop words.\n",
    "\n",
    "This has to be done to the stop words while they are still within the string because SpaCy uses grammatical context to lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list_lem = []\n",
    "\n",
    "sw_lem_Series = \\\n",
    "    df_new_nlp['spacy_text_comb'].apply(\n",
    "        lambda x: [t.lemma_.lower() for t in x if t.text.lower() in stopword_list]\n",
    "    )\n",
    "\n",
    "for row in sw_lem_Series:\n",
    "    stopword_list_lem.extend(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized tokens, excluding stop words\n",
    "\n",
    "df_new_nlp['text_comb_lemmas_no_sw'] = df_new_nlp['text_comb_lemmas'].apply(\n",
    "    lambda x: [l for l in x if l not in stopword_list_lem]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing most frequent lemmas without stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "top_50_lem = OrderedDict(\n",
    "    FreqDist(df_new_nlp[df_new_nlp['text_comb_lemmas_no_sw'].apply(lambda x: len(x) > 0)]['text_comb_lemmas_no_sw'].explode()).most_common(50)\n",
    "    )\n",
    "\n",
    "tokens = list(top_50_lem.keys())\n",
    "freq = list(top_50_lem.values())\n",
    "ax.bar(x=tokens, height=freq, color='#f14848')\n",
    "ax.set_xlabel('Lemmas', size=10)\n",
    "ax.set_ylabel('Frequency', size=10)\n",
    "ax.set_title('Top 50 lemmas in r/askreddit (no stop words)\\n2/9 to 2/14 2024')\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "fig.set_facecolor('white')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./images/top_50_lemmas_no_sw2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp[df_new_nlp['text_comb_lemmas'].apply(lambda x: 'taxis' in x)].loc[7]['text_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new_nlp[df_new_nlp['text_comb_lemmas'].apply(lambda x: 'taxis' in x)].loc[7]['text_comb_lemmas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poking around different frequent words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nlp[df_new_nlp['text_comb_tokens'].apply(lambda x: 'code' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where's my refund is a hot topic\n",
    "\n",
    "df_new_nlp[df_new_nlp['text_comb_tokens'].apply(lambda x: 'wmr' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(\n",
    "    # max_words=500, \n",
    "    # random_state=seed, \n",
    "    # font_path=\"./font/NotoSans-Regular.ttf\", \n",
    "    colormap='Blues',\n",
    "    # background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\",\n",
    "    background_color=\"black\", mode=\"RGBA\",\n",
    "    width=1500,\n",
    "    height=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_tokens = FreqDist(df_new_nlp['text_comb_tokens_no_sw'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_lemmas = FreqDist(df_new_nlp['text_comb_lemmas_no_sw'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate_from_frequencies(fd_tokens)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Frequent Words:\\nReddit r/askreddit Titles and Submission\\nTop posts of past year (as of 9/15/23)', fontsize=13)\n",
    "# plt.savefig('./images/word_cloud_tokens_text_comb2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.generate_from_frequencies(fd_lemmas)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Frequent Lemmas:\\nReddit r/askreddit Titles and Submission\\nTop posts of past year (as of 9/15/23)', fontsize=13)\n",
    "# plt.savefig('./images/word_cloud_lemmas_text_comb2.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can pull data like comment body and author name from the praw comment object.\n",
    "\n",
    "display(df_new['comments'].apply(lambda x: [top_level_comment.body for top_level_comment in x]))\n",
    "\n",
    "display(df_new['comments'].apply(lambda x: [top_level_comment.author for top_level_comment in x]))\n",
    "\n",
    "# Lots of GIFs in these comments that I should remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New df. Exploding the comments so each comment is its own row.\n",
    "# This is only showing top-level comments at this point\n",
    "\n",
    "df_comments_new_nlp = df_new[['id', 'title', 'datetime', 'comments']]\n",
    "df_comments_new_nlp['comments'] = df_new['comments'].apply(list)\n",
    "df_comments_new_nlp = df_comments_new_nlp.explode('comments')\n",
    "df_comments_new_nlp.columns = ['post_id', 'post_title', 'post_datetime', 'comment_object']\n",
    "df_comments_new_nlp = df_comments_new_nlp.reset_index().drop(columns='index')\n",
    "df_comments_new_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_new_nlp['comment_object'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_new_nlp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_new_nlp = df_comments_new_nlp.dropna(subset=['comment_object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column for comment text\n",
    "\n",
    "df_comments_new_nlp['comment'] = df_comments_new_nlp['comment_object'].apply(lambda x: x.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_new_nlp['comment'] = df_comments_new_nlp['comment'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates and useless comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_new_nlp['comment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop deleted and removed comments\n",
    "\n",
    "df_comments_new_nlp = df_comments_new_nlp[df_comments_new_nlp['comment'] != '[deleted]']\n",
    "df_comments_new_nlp = df_comments_new_nlp[df_comments_new_nlp['comment'] != '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop comments that are gifs\n",
    "\n",
    "df_comments_new_nlp = \\\n",
    "    df_comments_new_nlp[~df_comments_new_nlp['comment'].apply(lambda x: '![gif]' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop blank and single character comments\n",
    "\n",
    "df_comments_new_nlp = df_comments_new_nlp[~df_comments_new_nlp['comment'].apply(lambda x: len(x) in range(0,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a username column to aid in checking duplicates.\n",
    "\n",
    "df_comments_new_nlp['user'] = df_comments_new_nlp['comment_object'].apply(lambda x: x.author.name if x.author else 'no_user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate comments by the same user\n",
    "\n",
    "df_comments_new_nlp = df_comments_new_nlp.drop_duplicates(subset=['user', 'comment'], keep='faskredditt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop posts by AutoModerator, which is a moderator bot\n",
    "\n",
    "df_comments_new_nlp = df_comments_new_nlp[df_comments_new_nlp['user'] != 'AutoModerator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some duplicate comments may remain, but they are not by the same user so I will view them as unique\n",
    "\n",
    "display(df_comments_new_nlp[df_comments_new_nlp.duplicated(subset=['comment'])])\n",
    "\n",
    "display(df_comments_new_nlp[df_comments_new_nlp.duplicated(subset=['comment', 'user'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy object of each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
